services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:ollama
    container_name: open-webui
    restart: unless-stopped
    networks:
      - proxy
    extra_hosts:
      - host.docker.internal:host-gateway
    expose:
      - 8080
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=proxy"
      - "traefik.http.routers.openwebui.entrypoints=web,websecure"
      - "traefik.http.routers.openwebui.service=openwebui"
      - "traefik.http.routers.openwebui.tls=true"
      - "traefik.http.routers.openwebui.rule=Host(`openwebui.192.168.42.111.traefik.me`)"
      - "traefik.http.services.openwebui.loadbalancer.server.port=8080"
      - "homepage.group=AI Services"
      - "homepage.name=Open WebUI"
      - "homepage.icon=sh-open-webui-light"
      - "homepage.href=https://openwebui.192.168.42.111.traefik.me"
      - "homepage.description=Open WebUI Chat Interface"
    volumes:
      - open-webui:/app/backend/data
      - /etc/ssl/certs/:/etc/ssl/certs/:ro
      - /opt/workdir/llm-cache/huggingface:/llm-cache/huggingface
      - /opt/workdir/llm-cache/torchinductor:/llm-cache/torchinductor
      - /opt/workdir/llm-cache/ollama:/root/.ollama
    environment:
      - HF_HOME=/llm-cache/huggingface
      - TORCHINDUCTOR_CACHE_DIR=/llm-cache/torchinductor
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

volumes:
  open-webui:

networks:
  proxy:
    name: proxy
    external: true # enable if the network is not defined in this stack
